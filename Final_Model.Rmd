---
title: "Final model"
author: "Annie George"
date: "November 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readxl)
library(xlsx)
library(sqldf)
library(plyr)
library(tidyr)
library(caret)
library(janitor)
library(MASS)
library(fpp2)
library(car)
```


```{r}
sun=read_excel("clean_data.xlsx",  col_names=TRUE)
sun$average_customer_reviews_int = as.factor(round(sun$average_customer_review))
sun$title=as.factor(sun$title)
sun$asin=as.factor(sun$asin)
sun$upc_code=as.factor(sun$upc_code)
sun$product_group=as.factor(sun$product_group)
sun$category=as.factor(sun$category)
sun$subcategory=as.factor(sun$subcategory)
sun$replenishment_category=as.factor(sun$replenishment_category)
sun$platform1 = as.factor(sun$platform1)
sun$platform2 = as.factor(sun$platform2)
sun$platform_type = as.factor(sun$platform_type)
sun$national_holiday = as.factor(sun$national_holiday)

sun=sun[,!names(sun) %in% c("ordered_amount_trend", 
                            "ordered_amount_year_over_year_trend", 
                            "orders_trend", "orders_year_over_year_trend", 
                            "ordered_units_trend", 
                            "ordered_units_year_over_year_trend", 
                            "shipped_amount_trend", 
                            "shipped_amount_year_over_year_trend", 
                            "shipped_units_trend", 
                            "shipped_units_year_over_year_trend",  
                            "unfilled_customer_ordered_units_trend", 
                            "unfilled_customer_ordered_units_year_over_year_trend", 
                            "unique_visitors_trend", 
                            "unique_visitors_year_over_year_trend", 
                            "conversion_rate_trend", 
                            "conversion_rate_year_over_year_trend", 
                            "shipped_cost_of_goods_sold_trend", 
                            "shipped_cost_of_goods_sold_year_over_year_trend", 
                            "page_views_year_over_year_trend",  
                            "page_views_trend",
                            "conversion_percentile", 
                            "x1_week_forecast", 
                            "x4_week_forecast", 
                            "x12_week_forecast", 
                            "shipped_cost_of_goods_sold", 
                            "currency_code",
                            "platform1",
                            "platform2",
                            "transformed_order_units",
                            "brand")]
                          

sun=sun[, -c(1, 2)]
sun$week_start=as.Date(sun$week_start)
sun$week_end=as.Date(sun$week_end)
sun$release_date=as.Date(sun$release_date)
sun$month=as.factor(format(as.Date(sun$week_start), "%m"))
#write.xlsx(sun, "clean_week_2.xlsx" )
```

Get all the numerical variables in the dataset
```{r}
sun_new=sun
num_var = which(sapply(sun_new, is.numeric)) #index vector numeric variables
num_var= names(num_var) #saving names vector for use later on
```

Part II
#preserve ordered units and remove unfilled customer ordered units and sellable on hand cost
```{r}

features=num_var[-c(17)]
#scale all the numerical features
sun_new[,features]=scale(sun_new[,features])

sunPCA = prcomp(sun_new[,features], scale = TRUE)
plot(sunPCA, type="l") #variance at second component
summary(sunPCA)
sunPCA$rotation

vars_transformed = apply(sunPCA$x, 2, var)
vars_transformed
vars_transformed/sum(vars_transformed)  #PC1 explains 42% variation and PC2 explains 17% together explaining 59% PC3 explains 10% adding 69%

# First three principal components explains a lot
comp = data.frame(sunPCA$x[,1:3])
# Plot
plot(comp, pch=16, col=rgb(0,0,0.5))

sunpredictPCA = predict(sunPCA, sun_new[,features])
sunPCAcols <- cbind(sun_new, as.data.frame(sunpredictPCA)[,1:3])


ggplot(sunPCAcols, aes(x=PC1, y=PC2)) + 
    geom_point(aes(fill=upc_code),
               colour='black',
               pch=21,
               size=2) +
    theme_bw()

```


```{r}
set.seed(145)
library(factoextra)

fviz_nbclust(sunPCAcols[,c('PC1', 'PC2', 'PC3')], kmeans, method = "wss")  #2clusters 0r 5
fviz_nbclust(sunPCAcols[,c('PC1', 'PC2', 'PC3')], kmeans, method = "silhouette") #the second method points to 5
```

```{r}
kClusters = 3
kmeansPCA = kmeans(sunPCAcols[,c('PC1', 'PC2', 'PC3')], kClusters, nstart=25, iter.max = 1000)
ggplot(sunPCAcols, aes(x=PC1, y=PC2)) + 
    geom_point(aes(fill=factor(kmeansPCA$cluster)),
               colour='black',
               pch=21,
               size=2) +
    scale_fill_manual(values = c("black","red", 'green', 'blue', "purple", "orange","white")) +
    guides(fill=guide_legend(title="Cluster")) +
    theme_bw()
kmeansPCA$tot.withinss
library(cluster)
library(plotly)
clusplot(sun_new[,features], kmeansPCA$cluster, color=T, shade=F,labels=0,lines=0, main='k-Means Cluster Analysis')
rbind(unique(kmeansPCA$cluster), kmeansPCA$size)

```


```{r}
#library(dplyr)
library(reshape2)
sort(table(kmeansPCA$clust))
sunclust = names(sort(table(kmeansPCA$clust)))
sunCluster = sun_new[,c(features, "shipped_units", "title", "category", "subcategory", "upc_code", "asin", "platform_type", "replenishment_category", "week_number", "national_holiday", "month")]

cluster1=sunCluster[kmeansPCA$clust==sunclust[1],]
c1_uniq=as.data.frame(as.character(unique(cluster1$asin)))
names(c1_uniq) = "cluster1"
c1_uniq_week=as.character(sort(unique(cluster1$week_number)))

ggplot(data= melt(cluster1), aes(x=variable, y=value)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot(aes(fill=variable)) +
    coord_cartesian(ylim = c(0, 10)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none") 

cluster2=sunCluster[kmeansPCA$clust==sunclust[2],]
c2_uniq=as.data.frame(as.character(unique(cluster2$asin)))
names(c2_uniq) = "cluster2"
c2_uniq_week=as.character(sort(unique(cluster2$week_number)))

ggplot(data= melt(cluster2), aes(x=variable, y=value)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot(aes(fill=variable)) +
   coord_cartesian(ylim = c(0,7.5)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none") 

cluster3=sunCluster[kmeansPCA$clust==sunclust[3],]
c3_uniq=as.data.frame(as.character(unique(cluster3$asin)))
names(c3_uniq) = "cluster3"
c3_uniq_week=as.character(sort(unique(cluster3$week_number)))

ggplot(data= melt(cluster3), aes(x=variable, y=value)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot(aes(fill=variable)) +
    coord_cartesian(ylim = c(0, 4)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none") 

library(qpcR)
product_clusters=qpcR:::cbind.na(c1_uniq, c2_uniq, c3_uniq)
product_clusters

(product_clusters$cluster1)
length(product_clusters$cluster2)
length(product_clusters$cluster3)
length(product_clusters$cluster4)
length(product_clusters$cluster5)
length(product_clusters$cluster6)

PCA_cluster = cbind(sunCluster, kmeansPCA$cluster)
write.xlsx( PCA_cluster, "PCA_cluster.xlsx")
```

```{r}
c1=unique(cluster1$asin)
c2=unique(cluster2$asin)
c3=unique(cluster3$asin)
c1_not_in_c2 = unique(cluster1$asin[!cluster1$asin %in% c2])
c1_not_in_c3 = unique(cluster1$asin[!cluster1$asin %in% c3])

c2_not_in_c1 = unique(cluster2$asin[!cluster2$asin %in% c1])
common = unique(cluster2$asin[cluster2$asin %in% c1])
common13 = unique(cluster1$asin[cluster1$asin %in% c3])
common23 = unique(cluster2$asin[cluster2$asin %in% c3])

(c1_not_in_c2)
(c2_not_in_c1)
(c1_not_in_c3)

(common)
common13
common23
```

Part II - PCA with only important variables
#remove shipped units and trends
```{r}
train=sun_new[,-c(13, 17:29)]
num_var1 = which(sapply(train, is.numeric)) #index vector numeric variables
num_var1= names(num_var1) #saving names vector for use later on
 
features=num_var1
#scale all the numerical features
sun_new[,features]=scale(sun_new[,features])

sunPCA = prcomp(sun_new[,features], scale = TRUE)
plot(sunPCA, type="l") #variance at second component
summary(sunPCA)
sunPCA$rotation

vars_transformed = apply(sunPCA$x, 2, var)
vars_transformed
vars_transformed/sum(vars_transformed)  #PC1 explains 42% variation and PC2 explains 17% together explaining 59% PC3 explains 10% adding 69%

# First three principal components explains a lot
comp = data.frame(sunPCA$x[,1:2])
# Plot
plot(comp, pch=16, col=rgb(0,0,0.5))

sunpredictPCA = predict(sunPCA, sun_new[,features])
sunPCAcols <- cbind(sun_new, as.data.frame(sunpredictPCA)[,1:2])


ggplot(sunPCAcols, aes(x=PC1, y=PC2)) + 
    geom_point(aes(fill=asin),
               colour='black',
               pch=21,
               size=2) +
    theme_bw()

```


```{r}
set.seed(145)
library(factoextra)

fviz_nbclust(sunPCAcols[,c('PC1', 'PC2')], kmeans, method = "wss")  #2clusters 0r 5
fviz_nbclust(sunPCAcols[,c('PC1', 'PC2')], kmeans, method = "silhouette") #the second method points to 5
```

```{r}
kClusters = 3
kmeansPCA = kmeans(sunPCAcols[,c('PC1', 'PC2')], kClusters, nstart=25, iter.max = 1000)
ggplot(sunPCAcols, aes(x=PC1, y=PC2)) + 
    geom_point(aes(fill=factor(kmeansPCA$cluster)),
               colour='black',
               pch=21,
               size=2) +
    scale_fill_manual(values = c("black","red", 'green', 'blue', "purple", "orange","white")) +
    guides(fill=guide_legend(title="Cluster")) +
    theme_bw()
kmeansPCA$tot.withinss
library(cluster)
library(plotly)
clusplot(sun_new[,features], kmeansPCA$cluster, color=T, shade=F,labels=0,lines=0, main='k-Means Cluster Analysis')
kmeansPCA$size

```


```{r}
#library(dplyr)
library(reshape2)
sort(table(kmeansPCA$clust))
sunclust = names(sort(table(kmeansPCA$clust)))
sunCluster = sun_new[,c(features, "title", "category", "subcategory", "upc_code", "asin", "platform_type", "replenishment_category", "week_number", "national_holiday", "month")]

cluster1=sunCluster[kmeansPCA$clust==sunclust[1],]
c1_uniq=as.data.frame(as.character(unique(cluster1$title)))
names(c1_uniq) = "cluster1"
c1_uniq_week=as.character(sort(unique(cluster1$week_number)))

ggplot(data= melt(cluster1), aes(x=variable, y=value)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot(aes(fill=variable)) +
    coord_cartesian(ylim = c(0, 7)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none") 

cluster2=sunCluster[kmeansPCA$clust==sunclust[2],]
c2_uniq=as.data.frame(as.character(unique(cluster2$title)))
names(c2_uniq) = "cluster2"
c2_uniq_week=as.character(sort(unique(cluster2$week_number)))

ggplot(data= melt(cluster2), aes(x=variable, y=value)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot(aes(fill=variable)) +
   coord_cartesian(ylim = c(0,7.5)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none") 

cluster3=sunCluster[kmeansPCA$clust==sunclust[3],]
c3_uniq=as.data.frame(as.character(unique(cluster3$title)))
names(c3_uniq) = "cluster3"
c3_uniq_week=as.character(sort(unique(cluster3$week_number)))

ggplot(data= melt(cluster3), aes(x=variable, y=value)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot(aes(fill=variable)) +
    coord_cartesian(ylim = c(0, 5)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="none") 


library(qpcR)
product_clusters=qpcR:::cbind.na(c1_uniq, c2_uniq, c3_uniq, c4_uniq)
product_clusters

#p <- plot_ly(carsDf,x=Comp.1,y=Comp.2,text=rownames(carsDf),
#             mode="markers",color = cluster_name,marker=list(size=11))
#p <- layout(p,title="PCA Clusters from Hierachical Clustering of Cars Data",
#            xaxis=list(title="PC1"),
#            yaxis=list(title="PC2"))
(product_clusters$cluster1)
length(product_clusters$cluster2)
length(product_clusters$cluster3)
length(product_clusters$cluster4)
length(product_clusters$cluster5)
length(product_clusters$cluster6)

```

```{r}
c1=unique(cluster1$asin)
c2=unique(cluster2$asin)
c3=unique(cluster3$asin)
c1_not_in_c2 = unique(cluster1$asin[!cluster1$asin %in% c2])
c1_not_in_c3 = unique(cluster1$title[!cluster1$asin %in% c3])

c2_not_in_c1 = unique(cluster2$asin[!cluster2$asin %in% c1])
common = unique(cluster2$asin[cluster2$asin %in% c1])
common13 = unique(cluster1$asin[cluster1$asin %in% c3])
common23 = unique(cluster2$asin[cluster2$asin %in% c3])

(c1_not_in_c2)
(c2_not_in_c1)
(c1_not_in_c3)

(common)
common13
common23
```
Part I - Traditional models to find variable  importance
Finding correlated variables with the transformed order units
```{r}
library(psych)
library(caret)
library(GGally)
library(Hmisc)
set.seed(222)

sun_new=sun

ggcorr(sun_new[,num_var])
cor(as.matrix(sun_new[,num_var]))
  
cortest.bartlett(sun_new[,num_var], n=nrow(train))
str(sun_new)
```

```{r}
# Custom Control Parameters
custom = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       verboseIter = T)

```


Model 1: Elastic net  
1. Create dummy variable for use in model
2. Apply k-fold cross validation
3. The model will be reitrated to remove variables that are not significant to get the best model
4. Anova gives a better perspective of significance for the categorical variables 
List of variables removed -  title, week date start and end, first 2 columns, release date, platform1/2, upc code, asin, category, Results: lambda =0.00001, alpha =1 explains 85%
                80% variability is explained by 21 variables, hence only 20 top varaibles are chosen
```{r}
library(glmnet)
library(car)
dummy_model_en = dummyVars( ~ ., sun_new[,-c(42)]) #[,-c(4, 7, 8, 9, 37, 40)]) 
dummy_train_en= data.frame(predict(dummy_model_en, sun_new[,-c(42)])) #[, -c(4, 7, 8,9,  37, 40)])) 

en = train(shipped_units ~. -ordered_units 
           -average_ordered_price
           -amazon_shipped_sales_rank
           -ordered_amount
           -amazon_ordered_sales_rank
           -amazon_subcategory_ordered_units_rank
           -amazon_subcategory_shipped_sales_rank
           -amazon_category_shipped_units_rank
           -amazon_subcategory_shipped_units_rank
           -unfilled_customer_ordered_units
            -page_views_rank
           -shipped_amount
           -orders
           -unique_visitors_index
           -customer_reviews
           -sellable_on_hand_cost
           -unsellable_on_hand_cost
           -amazon_category_ordered_sales_rank
           -amazon_shipped_units_rank
           -amazon_ordered_units_rank,
               dummy_train_en ,
               method="glmnet",
               tuneGrid=expand.grid(alpha=seq(0,1,length=20),
                                    lambda=seq(0.000001, 1, length=50)),
             
             preProcess = c("center", "scale"),
              trControl=custom)

(en$results)
#plot(en$finalModel, xvar = 'dev', label=T)
plot(varImp(en), top=25)
durbinWatsonTest(residuals(en))
coef=coef(en$finalModel, s = en$bestTune$lambda)
coef
plot(residuals(en$finalModel))

#final en model
x=model.matrix(shipped_units ~. -ordered_units 
           -average_customer_review
           -amazon_shipped_sales_rank
           -ordered_amount
           -amazon_ordered_sales_rank
           -amazon_subcategory_ordered_units_rank
           -amazon_subcategory_shipped_sales_rank, data=sun_new)
y=sun_new$shipped_units

en_model=glmnet(x=x, y=y, alpha=en$bestTune[1], lambda=en$bestTune[2])
en_model$beta
en$results
```

```{r}
###
library(randomForest)
mtry <- sqrt(ncol(sun_new))
tunegrid <- expand.grid(.mtry=mtry)
smp_size <- floor(0.75 * nrow(sun_new))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(sun_new)), size = smp_size)

train <- sun_new[train_ind, ]
test <- sun_new[-train_ind, ]


rf_default <- train(shipped_units ~ . #-ordered_units - week_end - week_start  -title
                    #-orders -ordered_amount - shipped_amount
                    , data=sun_new, method="rf",  tuneGrid=tunegrid, 
                    trControl=custom, importance=TRUE)
print(rf_default)
imp=as.data.frame(importance(rf_default$finalModel, type=1))
ggplot(imp, aes(x=IncNodePurity, y=index2vec())) + geom_bar()
varImpPlot(rf_default$finalModel, sort=TRUE, type=1, top=30)
durbinWatsonTest(residuals(rf_default))


rf_train <- train(shipped_units ~ . -ordered_units - week_end - week_start  -title
                    -orders -ordered_amount - shipped_amount - release_date -average_ordered_price
                   -average_shipped_price -ordered_amount - sellable_on_hand_cost -unsellable_on_hand_cost
                  , data=train, method="rpart",  #tuneGrid=tunegrid, 
                    trControl=custom)    # , importance=TRUE)
library(car)
print(rf_train)
imp=as.data.frame(importance(rf_train$finalModel, type=1))
ggplot(imp, aes(x=IncNodePurity, y=index2vec())) + geom_bar()
varImpPlot(rf_train$finalModel, sort=TRUE, type=1, top=30)
durbinWatsonTest(residuals(rf_train))
residuals(rf_train)
```

Model 2: Stepwise  
```{r}
library(MASS)

step_model <- train(shipped_units ~., data = sun_new, 
method = "lmStepAIC", trControl = custom)

tail(step_model$results)
plot(step_model$finalModel, xvar = 'dev', label=T)
plot(varImp(step_model), top=25)
dwt(residuals(step_model))
coef=coef(step_model$finalModel, s = step_mode$bestTune$lambda)
coef
```

Model 1: Linear Model using the dummy train
1. Create dummy variable for use in model
2. Apply k-fold cross validation
3. The model will be reitrated to remove variables that are not significant to get the best model
4. Anova gives a better perspective of significance for the categorical variables 
Remove
"unfilled_customer_ordered_units",
"page_views_index"
page_views_rank
customer_reviews
```{r}
dummy_model_lm = dummyVars( ~.,  sun_new[,-c(1:9,41:46)]) #[,-c(37, 39)])
dummy_train_lm = data.frame(predict(dummy_model_lm, sun_new[,-c(1:9, 41:46)])) #[,-c(37, 39)]))

dummy_model_lm = dummyVars( ~.,  train[,-c(43,5)]) #[,-c(37, 39)])
dummy_train_lm = data.frame(predict(dummy_model_lm, train[,-c(43,5)])) #[,-c(37, 39)]))

dummy_test_lm = dummyVars( ~.,  test[,-c(43,5)]) #[,-c(37, 39)])
dummy_test_lm1 = data.frame(predict(dummy_test_lm, test[,-c(43,5)])) #[,-c(37, 39)]))


lm = train(shipped_units ~ . -ordered_units   
                    -orders -ordered_amount - shipped_amount - average_ordered_price
                   -average_shipped_price -ordered_amount - sellable_on_hand_cost -unsellable_on_hand_cost
           -amazon_subcategory_ordered_units_rank
           -amazon_subcategory_shipped_units_rank
           -amazon_category_ordered_units_rank -amazon_shipped_units_rank
           -amazon_category_ordered_sales_rank -amazon_shipped_sales_rank
           -amazon_category_shipped_sales_rank
           -amazon_category_shipped_units_rank
           -amazon_subcategory_shipped_sales_rank
            -unfilled_customer_ordered_units 
           -average_customer_review -unsellable_on_hand_units
           -amazon_subcategory_ordered_sales_rank
           -list_price
           -open_purchase_order_quantity,
            dummy_train_lm,
            method="lm",
            preProcess = c("center", "scale"),
            trControl=custom)

summary(lm$finalModel)
anova(lm$finalModel)
durbinWatsonTest(residuals(lm))
summary(lm)
lm$results
plot(lm$finalModel)
plot(varImp(lm), top=20)
lm$finalModel
names(sun_new)


y.test=predict(lm$finalModel, dummy_test_lm1)
rmse=mean(sum(y.test-test$shipped_units)^2)
```

Model 2: Lasso Model using the dummy train
1. Create dummy variable for use in model
2. Apply k-fold cross validation
3. The model will be reitrated to remove variables that are not significant to get the best model
4. Anova gives a better perspective of significance for the categorical variables 

```{r}
names(train)
dummy_model_las = dummyVars( ~ . , sun_new[,])
dummy_train_las= data.frame(predict(dummy_model_las, sun_new[,-c(37, 7)]))

lasso = train(shipped_units ~.  -customer_reviews  -page_views_rank 
              -amazon_category_ordered_units_rank 
              -amazon_subcategory_ordered_units_rank
              -unsellable_on_hand_units
              -sellable_on_hand_cost
              -amazon_shipped_units_rank                                                               
              -amazon_category_shipped_units_rank                     
              - amazon_subcategory_shipped_units_rank
              -sellable_on_hand_units 
              -amazon_ordered_sales_rank
              -average_ordered_price
              -list_price,  
               dummy_train_las,
               method="glmnet",
               tuneGrid=expand.grid(alpha=1,
                                    lambda=seq(0.0000001, 1, length=150)),
              preProcess = c("center", "scale"),
              trControl=custom)
coef(lasso$finalModel, s = lasso$bestTune$lambda)
plot(lasso)
lasso$results

plot(lasso$finalModel, xvar = 'dev', label=T)
plot(varImp(lasso, scale=F), top=20)
dwt(residuals(lasso))

```



Compare Models 
 The best model is Elastic net which explains 86% variatiom
```{r}
model_list = list(LinearModel=lm, Elasticnet=en, Randomforest=rf_default)
res = resamples(model_list)
summary(res)
```
Durbin watson
```{r}
library(car)
dw_list = list(LinearModel=dwt(residuals(lm)), 
               Elasticnet=dwt(residuals(en)), 
               Randomforest=dwt(residuals(rf_default)))
dw_list
```



```{r}
library(glmnet)
lm$bestTune

saveRDS(en, "final_model_lm.rds")
fm = readRDS("final_model_lm.rds")

final_x= sun_new

dummy_final= model.matrix( ~ ., final_x[,-c(39, 43)])
lm = lm(diff_units ~. -sellable_on_hand_cost  
                        -sellable_on_hand_units
                        - list_price
                        -average_shipped_price
-amazon_ordered_sales_rank
-amazon_category_ordered_units_rank
-amazon_shipped_units_rank
-amazon_subcategory_shipped_units_rank
-average_customer_review
-page_views_rank
-page_views_index
-unique_visitors_index
-open_purchase_order_quantity
-release_date
-amazon_category_ordered_sales_rank
-amazon_shipped_sales_rank
-amazon_category_shipped_sales_rank
-amazon_subcategory_shipped_sales_rank 
-unfilled_customer_ordered_units
-amazon_subcategory_ordered_sales_rank
-amazon_ordered_units_rank
-amazon_category_shipped_units_rank, data=sun_new[,-c(39)])

anova(lm)
plot(residuals(lasso))
plot(predict(lasso), residuals(lasso)) #no constant variance
qqplot(predict(lasso),residuals(lasso))
hist(residuals(lasso),main='Histogram of residuals',xlab='Standardised
Residuals',ylab='Frequency')


final$beta
plot(varImp(lasso), top=15)


```
#IF en IS THE BEST 

```{r}
library(glmnet)
lasso$bestTune

# Save Final Model for Later Use
saveRDS(lasso, "final_model_lass.rds")
fm = readRDS("final_model_lass.rds")

final_x= sun_new

dummy_final= model.matrix( ~ ., final_x)
final=glmnet(x=dummy_final, y=final_x$shipped_units, family = "gaussian", lambda = lasso$bestTune[2], alpha = lasso$bestTune[1])

plot(residuals(lasso))
plot(predict(lasso), residuals(lasso)) #no constant variance
qqplot(predict(lasso),residuals(lasso))
hist(residuals(lasso),main='Histogram of residuals',xlab='Standardised
Residuals',ylab='Frequency')
summary(lasso$finalModel$param)

plot(varImp(lasso), top=20)
(final$lambda)


```

```{r}

ggplot(sun, aes(x=sort(week_number), y=diff_units, col=category)) + geom_boxplot() +
  ylim(c(-15,15)) +
  geom_hline(yintercept=0,col="black") +
  ylab("Difference between Ordered Units and Shipment Units") +
  xlab("week number")

ggplot(sun_new, aes(x=sort(week_number), y=diff_units, col=subcategory)) + geom_boxplot() +
  ylim(c(-15,15)) +
  geom_hline(yintercept=0,col="black")

tit_subcat=unique(sun[,c("title", "subcategory", "category")])
table("subcategory"=tit_subcat$category, "category"=tit_subcat$subcategory) 


tit_subcat$title[tit_subcat$subcategory == "8030 Face"]
tit_subcat$title[tit_subcat$category == "0900 GM/HBC"]
 
unique(sun$title[(as.numeric(sun$week_number) >="12" & as.numeric(sun$week_number) >="16") & sun$subcategory == "8090 Sun"])

ggplot(sun, aes(x=as.factor(platform_type), fill=category)) + geom_bar() +
  ylim(c(0,1000)) +  #2 products under 8030 face is popular between 30 and 34th week, why?
  xlab("Platforms") 

ggplot(sun, aes(x=as.factor(platform_type), fill=subcategory)) + geom_bar() +
  ylim(c(0,1000)) +  #2 products under 8030 face is popular between 30 and 34th week, why?
  xlab("Platforms") 

```
